{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradEnv(gym.Env):\n",
    "    def __init__(self, n_discr, df, max_step):\n",
    "        self.n_discr = n_discr\n",
    "        self.df = df\n",
    "        self.max_step = max_step\n",
    "        low = np.zeros(52)\n",
    "        high = 1 + low\n",
    "        self.observation_space = spaces.Box(low=low, high=high, dtype=np.float32)\n",
    "        self.action_space = spaces.Discrete(n_discr)\n",
    "        self.num_envs = 4\n",
    "        self.reset()\n",
    "    \n",
    "    def step(self, action):\n",
    "        s_ = np.zeros(52)\n",
    "        s_[:2] = np.array([action/self.n_discr, 1 - (action/self.n_discr)])\n",
    "        s_[2:] = self.get_window()\n",
    "        r = self.state[:2] @ np.array([1, self.df[\"y\"][self.steps]])\n",
    "        done = (self.steps < self.max_step)\n",
    "        self.steps += 1\n",
    "        return s_, r, done, None\n",
    "        \n",
    "    def get_window(self):\n",
    "        return np.zeros(50)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.steps = 0\n",
    "        self.profit = 1\n",
    "        self.candle = 0\n",
    "        self.state = np.zeros(52)\n",
    "        self.state[0] = 1\n",
    "        return self.state\n",
    "        \n",
    "    def render(self):\n",
    "        return\n",
    "        \n",
    "    def close(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([0.1, 0.4, -0.5], columns=[\"y\"])\n",
    "env = TradEnv(10, df, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ziyed/.venv/psc/lib/python3.6/site-packages/tensorflow/python/ops/clip_ops.py:110: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 99       |\n",
      "| episodes                | 10       |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 8        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 99       |\n",
      "| episodes                | 20       |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 18       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 99       |\n",
      "| episodes                | 30       |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 28       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 99       |\n",
      "| episodes                | 40       |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 38       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 99       |\n",
      "| episodes                | 50       |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 48       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 99       |\n",
      "| episodes                | 60       |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 58       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 99       |\n",
      "| episodes                | 70       |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 68       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 99       |\n",
      "| episodes                | 80       |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 78       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 99       |\n",
      "| episodes                | 90       |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 88       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 99       |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 98       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 98       |\n",
      "| episodes                | 110      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 108      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 98       |\n",
      "| episodes                | 120      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 118      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 98       |\n",
      "| episodes                | 130      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 128      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 98       |\n",
      "| episodes                | 140      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 138      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 98       |\n",
      "| episodes                | 150      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 148      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 98       |\n",
      "| episodes                | 160      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 158      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 98       |\n",
      "| episodes                | 170      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 168      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 98       |\n",
      "| episodes                | 180      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 178      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 98       |\n",
      "| episodes                | 190      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 188      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 98       |\n",
      "| episodes                | 200      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 198      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 97       |\n",
      "| episodes                | 210      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 208      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 97       |\n",
      "| episodes                | 220      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 218      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 97       |\n",
      "| episodes                | 230      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 228      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 97       |\n",
      "| episodes                | 240      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 238      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 97       |\n",
      "| episodes                | 250      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 248      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 97       |\n",
      "| episodes                | 260      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 258      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 97       |\n",
      "| episodes                | 270      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 268      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 97       |\n",
      "| episodes                | 280      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 278      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 97       |\n",
      "| episodes                | 290      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 288      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 97       |\n",
      "| episodes                | 300      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 298      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 96       |\n",
      "| episodes                | 310      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 308      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 96       |\n",
      "| episodes                | 320      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 318      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 96       |\n",
      "| episodes                | 330      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 328      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 96       |\n",
      "| episodes                | 340      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 338      |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 96       |\n",
      "| episodes                | 350      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 348      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 96       |\n",
      "| episodes                | 360      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 358      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 96       |\n",
      "| episodes                | 370      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 368      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 96       |\n",
      "| episodes                | 380      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 378      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 96       |\n",
      "| episodes                | 390      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 388      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 96       |\n",
      "| episodes                | 400      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 398      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 96       |\n",
      "| episodes                | 410      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 408      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 95       |\n",
      "| episodes                | 420      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 418      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 95       |\n",
      "| episodes                | 430      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 428      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 95       |\n",
      "| episodes                | 440      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 438      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 95       |\n",
      "| episodes                | 450      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 448      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 95       |\n",
      "| episodes                | 460      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 458      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 95       |\n",
      "| episodes                | 470      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 468      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 95       |\n",
      "| episodes                | 480      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 478      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 95       |\n",
      "| episodes                | 490      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 488      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 95       |\n",
      "| episodes                | 500      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 498      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 95       |\n",
      "| episodes                | 510      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 508      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 94       |\n",
      "| episodes                | 520      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 518      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 94       |\n",
      "| episodes                | 530      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 528      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 94       |\n",
      "| episodes                | 540      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 538      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 94       |\n",
      "| episodes                | 550      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 548      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 94       |\n",
      "| episodes                | 560      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 558      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 94       |\n",
      "| episodes                | 570      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 568      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 94       |\n",
      "| episodes                | 580      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 578      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 94       |\n",
      "| episodes                | 590      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 588      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 94       |\n",
      "| episodes                | 600      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 598      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 94       |\n",
      "| episodes                | 610      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 608      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 93       |\n",
      "| episodes                | 620      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 618      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 93       |\n",
      "| episodes                | 630      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 628      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 93       |\n",
      "| episodes                | 640      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 638      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 93       |\n",
      "| episodes                | 650      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 648      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 93       |\n",
      "| episodes                | 660      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 658      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 93       |\n",
      "| episodes                | 670      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 668      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 93       |\n",
      "| episodes                | 680      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 678      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 93       |\n",
      "| episodes                | 690      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 688      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 93       |\n",
      "| episodes                | 700      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 698      |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 93       |\n",
      "| episodes                | 710      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 708      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 92       |\n",
      "| episodes                | 720      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 718      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 92       |\n",
      "| episodes                | 730      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 728      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 92       |\n",
      "| episodes                | 740      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 738      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 92       |\n",
      "| episodes                | 750      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 748      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 92       |\n",
      "| episodes                | 760      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 758      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 92       |\n",
      "| episodes                | 770      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 768      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 92       |\n",
      "| episodes                | 780      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 778      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 92       |\n",
      "| episodes                | 790      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 788      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 92       |\n",
      "| episodes                | 800      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 798      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 92       |\n",
      "| episodes                | 810      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 808      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 91       |\n",
      "| episodes                | 820      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 818      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 91       |\n",
      "| episodes                | 830      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 828      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 91       |\n",
      "| episodes                | 840      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 838      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 91       |\n",
      "| episodes                | 850      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 848      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 91       |\n",
      "| episodes                | 860      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 858      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 91       |\n",
      "| episodes                | 870      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 868      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 91       |\n",
      "| episodes                | 880      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 878      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 91       |\n",
      "| episodes                | 890      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 888      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 91       |\n",
      "| episodes                | 900      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 898      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 91       |\n",
      "| episodes                | 910      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 908      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 91       |\n",
      "| episodes                | 920      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 918      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 90       |\n",
      "| episodes                | 930      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 928      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 90       |\n",
      "| episodes                | 940      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 938      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 90       |\n",
      "| episodes                | 950      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 948      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 90       |\n",
      "| episodes                | 960      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 958      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 90       |\n",
      "| episodes                | 970      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 968      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 90       |\n",
      "| episodes                | 980      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 978      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 90       |\n",
      "| episodes                | 990      |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 988      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 90       |\n",
      "| episodes                | 1000     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 998      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 90       |\n",
      "| episodes                | 1010     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1008     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 90       |\n",
      "| episodes                | 1020     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1018     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 89       |\n",
      "| episodes                | 1030     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1028     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 89       |\n",
      "| episodes                | 1040     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1038     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 89       |\n",
      "| episodes                | 1050     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1048     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 89       |\n",
      "| episodes                | 1060     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1058     |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 89       |\n",
      "| episodes                | 1070     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1068     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 89       |\n",
      "| episodes                | 1080     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1078     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 89       |\n",
      "| episodes                | 1090     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1088     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 89       |\n",
      "| episodes                | 1100     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1098     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 89       |\n",
      "| episodes                | 1110     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1108     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 89       |\n",
      "| episodes                | 1120     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1118     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 88       |\n",
      "| episodes                | 1130     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1128     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 88       |\n",
      "| episodes                | 1140     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1138     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 88       |\n",
      "| episodes                | 1150     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1148     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 88       |\n",
      "| episodes                | 1160     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1158     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 88       |\n",
      "| episodes                | 1170     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1168     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 88       |\n",
      "| episodes                | 1180     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1178     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 88       |\n",
      "| episodes                | 1190     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1188     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 88       |\n",
      "| episodes                | 1200     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1198     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 88       |\n",
      "| episodes                | 1210     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1208     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 88       |\n",
      "| episodes                | 1220     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1218     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 87       |\n",
      "| episodes                | 1230     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1228     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 87       |\n",
      "| episodes                | 1240     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1238     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 87       |\n",
      "| episodes                | 1250     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1248     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 87       |\n",
      "| episodes                | 1260     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1258     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 87       |\n",
      "| episodes                | 1270     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1268     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 87       |\n",
      "| episodes                | 1280     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1278     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 87       |\n",
      "| episodes                | 1290     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1288     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 87       |\n",
      "| episodes                | 1300     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1298     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 87       |\n",
      "| episodes                | 1310     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1308     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 87       |\n",
      "| episodes                | 1320     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1318     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 86       |\n",
      "| episodes                | 1330     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1328     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 86       |\n",
      "| episodes                | 1340     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1338     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 86       |\n",
      "| episodes                | 1350     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1348     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 86       |\n",
      "| episodes                | 1360     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1358     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 86       |\n",
      "| episodes                | 1370     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1368     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 86       |\n",
      "| episodes                | 1380     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1378     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 86       |\n",
      "| episodes                | 1390     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1388     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 86       |\n",
      "| episodes                | 1400     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1398     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 86       |\n",
      "| episodes                | 1410     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1408     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 86       |\n",
      "| episodes                | 1420     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1418     |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 86       |\n",
      "| episodes                | 1430     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1428     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 85       |\n",
      "| episodes                | 1440     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1438     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 85       |\n",
      "| episodes                | 1450     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1448     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 85       |\n",
      "| episodes                | 1460     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1458     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 85       |\n",
      "| episodes                | 1470     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1468     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 85       |\n",
      "| episodes                | 1480     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1478     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 85       |\n",
      "| episodes                | 1490     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1488     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 85       |\n",
      "| episodes                | 1500     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1498     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 85       |\n",
      "| episodes                | 1510     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1508     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 85       |\n",
      "| episodes                | 1520     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1518     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 85       |\n",
      "| episodes                | 1530     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1528     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 84       |\n",
      "| episodes                | 1540     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1538     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 84       |\n",
      "| episodes                | 1550     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1548     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 84       |\n",
      "| episodes                | 1560     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1558     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 84       |\n",
      "| episodes                | 1570     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1568     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 84       |\n",
      "| episodes                | 1580     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1578     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 84       |\n",
      "| episodes                | 1590     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1588     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 84       |\n",
      "| episodes                | 1600     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1598     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 84       |\n",
      "| episodes                | 1610     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1608     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 84       |\n",
      "| episodes                | 1620     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1618     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 84       |\n",
      "| episodes                | 1630     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1628     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 83       |\n",
      "| episodes                | 1640     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1638     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 83       |\n",
      "| episodes                | 1650     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1648     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 83       |\n",
      "| episodes                | 1660     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1658     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 83       |\n",
      "| episodes                | 1670     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1668     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 83       |\n",
      "| episodes                | 1680     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1678     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 83       |\n",
      "| episodes                | 1690     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1688     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 83       |\n",
      "| episodes                | 1700     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1698     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 83       |\n",
      "| episodes                | 1710     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1708     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 83       |\n",
      "| episodes                | 1720     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1718     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 83       |\n",
      "| episodes                | 1730     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1728     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 82       |\n",
      "| episodes                | 1740     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1738     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 82       |\n",
      "| episodes                | 1750     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1748     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 82       |\n",
      "| episodes                | 1760     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1758     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 82       |\n",
      "| episodes                | 1770     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1768     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 82       |\n",
      "| episodes                | 1780     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1778     |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 82       |\n",
      "| episodes                | 1790     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1788     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 82       |\n",
      "| episodes                | 1800     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 1798     |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ffe7c190086e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mexploration_final_eps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     )\n",
      "\u001b[0;32m~/github/trad/baselines/deepq/simple.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(env, q_func, lr, max_timesteps, buffer_size, exploration_fraction, exploration_final_eps, train_freq, batch_size, print_freq, checkpoint_freq, learning_starts, gamma, target_network_update_freq, prioritized_replay, prioritized_replay_alpha, prioritized_replay_beta0, prioritized_replay_beta_iters, prioritized_replay_eps, param_noise, callback)\u001b[0m\n\u001b[1;32m    261\u001b[0m                     \u001b[0mobses_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobses_tp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                     \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m                 \u001b[0mtd_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobses_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobses_tp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mprioritized_replay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                     \u001b[0mnew_priorities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtd_errors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprioritized_replay_eps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/trad/baselines/common/tf_util.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgivens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgivens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/trad/baselines/common/tf_util.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minpt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgivens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minpt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgivens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minpt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs_update\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venv/psc/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venv/psc/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venv/psc/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venv/psc/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venv/psc/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from baselines import deepq\n",
    "\n",
    "\n",
    "model = deepq.models.mlp([64])\n",
    "act = deepq.learn(\n",
    "        env,\n",
    "        q_func=model,\n",
    "        lr=1e-3,\n",
    "        max_timesteps=100000,\n",
    "        buffer_size=50000,\n",
    "        exploration_fraction=0.1,\n",
    "        exploration_final_eps=0.02,\n",
    "        print_freq=10,\n",
    "        callback=None\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psc",
   "language": "python",
   "name": "psc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
